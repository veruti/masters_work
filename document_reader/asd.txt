Fine-tuneBERTforExtractiveSummarizationYangLiuInstituteforLanguage,CognitionandComputationSchoolofInformatics,UniversityofEdinburgh10CrichtonStreet,EdinburghEH89AByang.liu2@ed.ac.ukAbstractBERT(Devlinetal.,2018),apre-trainedTransformer(Vaswanietal.,2017)model,hasachievedground-breakingperformanceonmultipleNLPtasks.Inthispaper,wedescribeBERTSUM,asimplevariantofBERT,forex-tractivesummarization.OursystemisthestateoftheartontheCNN/Dailymaildataset,out-performingthepreviousbest-performedsys-temby1.65onROUGE-L.Thecodestorepro-duceourresultsareavailableathttps://github.com/nlpyang/BertSum1IntroductionSingle-documentsummarizationisthetaskofau-tomaticallygeneratingashorterversionofadoc-umentwhileretainingitsmostimportantinforma-tion.Thetaskhasreceivedmuchattentioninthenaturallanguageprocessingcommunityduetoitspotentialforvariousinformationaccessapplica-tions.Examplesincludetoolswhichdigesttextualcontent(e.g.,news,socialmedia,reviews),answerquestions,orproviderecommendations.Thetaskisoftendividedintotwoparadigms,abstractivesummarizationandextractivesumma-rization.Inabstractivesummarization,targetsum-mariescontainswordsorphrasesthatwerenotintheoriginaltextandusuallyrequirevarioustextrewritingoperationstogenerate,whileextractiveapproachesformsummariesbycopyingandcon-catenatingthemostimportantspans(usuallysen-tences)inadocument.Inthispaper,wefocusonextractivesummarization.Althoughmanyneuralmodelshavebeenproposedforextractivesummarizationre-cently(ChengandLapata,2016;Nallapatietal.,2017;Narayanetal.,2018;Dongetal.,2018;Zhangetal.,2018;Zhouetal.,2018),theim-provementonautomaticmetricslikeROUGE†Pleaseseehttps://arxiv.org/abs/1908.08345forthefullandmostcurrentversionofthispaperhasreachedabottleneckduetothecomplex-ityofthetask.Inthispaper,wearguethat,BERT(Devlinetal.,2018),withitspre-trainingonahugedatasetandthepowerfularchitectureforlearningcomplexfeatures,canfurtherboosttheperformanceofextractivesummarization.Inthispaper,wefocusondesigningdiffer-entvariantsofusingBERTontheextractivesummarizationtaskandshowingtheirresultsonCNN/DailymailandNYTdatasets.Wefoundthataﬂatarchitecturewithinter-sentenceTransformerlayersperformsthebest,achievingthestate-of-the-artresultsonthistask.2MethodologyLetddenoteadocumentcontainingseveralsen-tences[sent1,sent2,···,sentm],wheresentiisthei-thsentenceinthedocument.Extractivesum-marizationcanbedeﬁnedasthetaskofassigningalabelyi∈{0,1}toeachsenti,indicatingwhetherthesentenceshouldbeincludedinthesummary.Itisassumedthatsummarysentencesrepresentthemostimportantcontentofthedocument.2.1ExtractiveSummarizationwithBERTTouseBERTforextractivesummarization,werequireittooutputtherepresentationforeachsentence.However,sinceBERTistrainedasamasked-languagemodel,theoutputvectorsaregroundedtotokensinsteadofsentences.Mean-while,althoughBERThassegmentationembed-dingsforindicatingdifferentsentences,itonlyhastwolabels(sentenceAorsentenceB),insteadofmultiplesentencesasinextractivesummarization.Therefore,wemodifytheinputsequenceandem-beddingsofBERTtomakeitpossibleforextract-ingsummaries.EncodingMultipleSentencesAsillustratedinFigure1,weinserta[CLS]tokenbeforeeachsen-arXiv:1903.10318v2  [cs.CL]  5 Sep 2019sentone2ndsentsentagain[CLS][SEP][CLS][SEP][CLS][SEP]E[CLS]EsentEoneE[SEP]E[CLS]E2ndEsentE[SEP]E[CLS]EsentEagainE[SEP]EAEAEAEAEBEBEBEBEAEAEAEAE1E2E3E4E5E6E7E8E9E10E11E12T1T2T3BERT..................Summarization LayersY1Y2Y3Input DocumentToken EmbeddingsInterval Segment EmbeddingsPosition EmbeddingsFigure1:TheoverviewarchitectureoftheBERTSUMmodel.tenceanda[SEP]tokenaftereachsentence.InvanillaBERT,The[CLS]isusedasasymboltoaggregatefeaturesfromonesentenceorapairofsentences.Wemodifythemodelbyusingmul-tiple[CLS]symbolstogetfeaturesforsentencesascendingthesymbol.IntervalSegmentEmbeddingsWeuseinter-valsegmentembeddingstodistinguishmultiplesentenceswithinadocument.ForsentiwewillassignasegmentembeddingEAorEBcondi-tionedoniisoddoreven.Forexample,for[sent1,sent2,sent3,sent4,sent5]wewillassign[EA,EB,EA,EB,EA].ThevectorTiwhichisthevectorofthei-th[CLS]symbolfromthetopBERTlayerwillbeusedastherepresentationforsenti.2.2Fine-tuningwithSummarizationLayersAfterobtainingthesentencevectorsfromBERT,webuildseveralsummarization-speciﬁclayersstackedontopoftheBERToutputs,tocapturedocument-levelfeaturesforextractingsummaries.Foreachsentencesenti,wewillcalculatetheﬁ-nalpredictedscoreˆYi.ThelossofthewholemodelistheBinaryClassiﬁcationEntropyofˆYiagainstgoldlabelYi.Thesesummarizationlayersarejointlyﬁne-tunedwithBERT.SimpleClassiﬁerLikeintheoriginalBERTpa-per,theSimpleClassiﬁeronlyaddsalinearlayerontheBERToutputsanduseasigmoidfunctiontogetthepredictedscore:ˆYi=σ(WoTi+bo)(1)whereσistheSigmoidfunction.Inter-sentenceTransformerInsteadofasim-plesigmoidclassiﬁer,Inter-sentenceTransformerappliesmoreTransformerlayersonlyonsen-tencerepresentations,extractingdocument-levelfeaturesfocusingonsummarizationtasksfromtheBERToutputs:˜hl=LN(hl−1+MHAtt(hl−1))(2)hl=LN(˜hl+FFN(˜hl))(3)whereh0=PosEmb(T)andTarethesen-tencevectorsoutputbyBERT,PosEmbisthefunctionofaddingpositionalembeddings(indi-catingthepositionofeachsentence)toT;LNisthelayernormalizationoperation(Baetal.,2016);MHAttisthemulti-headattentionoper-ation(Vaswanietal.,2017);thesuperscriptlindi-catesthedepthofthestackedlayer.Theﬁnaloutputlayerisstillasigmoidclassi-ﬁer:ˆYi=σ(WohLi+bo)(4)wherehListhevectorforsentifromthetoplayer(theL-thlayer)oftheTransformer.Inexperiments,weimplementedTransformerswithL=1,2,3andfoundTransformerwith2layersperformsthebest.RecurrentNeuralNetworkAlthoughtheTransformermodelachievedgreatresultsonseveraltasks,thereareevidencethatRecurrentNeuralNetworksstillhavetheiradvantages,especiallywhencombiningwithtechniquesinTransformer(Chenetal.,2018).Therefore,weapplyanLSTMlayerovertheBERToutputstolearnsummarization-speciﬁcfeatures.Tostabilizethetraining,pergatelayernormal-ization(Baetal.,2016)isappliedwithineachLSTMcell.Attimestepi,theinputtotheLSTMlayeristheBERToutputTi,andtheoutputiscal-culatedas:FiIiOiGi=LNh(Whhi−1)+LNx(WxTi)(5)Ci=σ(Fi)(cid:12)Ci−1+σ(Ii)(cid:12)tanh(Gi−1)(6)hi=σ(Ot)(cid:12)tanh(LNc(Ct))(7)whereFi,Ii,Oiareforgetgates,inputgates,outputgates;GiisthehiddenvectorandCiisthememoryvector;hiistheoutputvector;LNh,LNx,LNcaretheredifferencelayernormal-izationoperations;Biastermsarenotshown.Theﬁnaloutputlayerisalsoasigmoidclassi-ﬁer:ˆYi=σ(Wohi+bo)(8)3ExperimentsInthissectionwepresentourimplementation,de-scribethesummarizationdatasetsandourevalua-tionprotocol,andanalyzeourresults.3.1ImplementationDetailsWeusePyTorch,OpenNMT(Kleinetal.,2017)andthe‘bert-base-uncased’∗versionofBERTtoimplementthemodel.BERTandsummarizationlayersarejointlyﬁne-tuned.Adamwithβ1=0.9,β2=0.999isusedforﬁne-tuning.Learningratescheduleisfollowing(Vaswanietal.,2017)withwarming-uponﬁrst10,000steps:lr=2e−3·min(step−0.5,step·warmup−1.5)Allmodelsaretrainedfor50,000stepson3GPUs(GTX1080Ti)withgradientaccumulation∗https://github.com/huggingface/pytorch-pretrained-BERTpertwosteps,whichmakesthebatchsizeapprox-imatelyequalto36.Modelcheckpointsaresavedandevaluatedonthevalidationsetevery1,000steps.Weselectthetop-3checkpointsbasedontheirevaluationlossesonthevalidationsset,andreporttheaveragedresultsonthetestset.Whenpredictingsummariesforanewdocu-ment,weﬁrstusethemodelstoobtainthescoreforeachsentence.Wethenrankthesesentencesbythescoresfromhighertolower,andselectthetop-3sentencesasthesummary.TrigramBlockingDuringthepredictingpro-cess,TrigramBlockingisusedtoreduceredun-dancy.GivenselectedsummarySandacandidatesentencec,wewillskipcisthereexistsatrigramoverlappingbetweencandS.ThisissimilartotheMaximalMarginalRelevance(MMR)(CarbonellandGoldstein,1998)butmuchsimpler.3.2SummarizationDatasetsWeevaluatedontwobenchmarkdatasets,namelytheCNN/DailyMailnewshighlightsdataset(Her-mannetal.,2015)andtheNewYorkTimesAnnotatedCorpus(NYT;Sandhaus2008).TheCNN/DailyMaildatasetcontainsnewsarticlesandassociatedhighlights,i.e.,afewbulletpointsgiv-ingabriefoverviewofthearticle.WeusedthestandardsplitsofHermannetal.(2015)fortrain-ing,validation,andtesting(90,266/1,220/1,093CNNdocumentsand196,961/12,148/10,397Dai-lyMaildocuments).Wedidnotanonymizeenti-ties.WeﬁrstsplitsentencesbyCoreNLPandpre-processthedatasetfollowingmethodsinSeeetal.(2017).TheNYTdatasetcontains110,540articleswithabstractivesummaries.FollowingDurrettetal.(2016),wesplittheseinto100,834trainingand9,706testexamples,basedondateofpublication(testisallarticlespublishedonJanuary1,2007orlater).Wetook4,000examplesfromthetrainingsetasthevalidationset.Wealsofollowedtheirﬁl-teringprocedure,documentswithsummariesthatareshorterthan50wordswereremovedfromtherawdataset.Theﬁlteredtestset(NYT50)in-cludes3,452testexamples.Weﬁrstsplitsen-tencesbyCoreNLPandpre-processthedatasetfollowingmethodsinDurrettetal.(2016).Bothdatasetscontainabstractivegoldsum-maries,whicharenotreadilysuitedtotrainingextractivesummarizationmodels.Agreedyalgo-rithmwasusedtogenerateanoraclesummaryforModelROUGE-1ROUGE-2ROUGE-LPGN∗39.5317.2837.98DCA∗41.6919.4737.92LEAD40.4217.6236.67ORACLE52.5931.2448.87REFRESH∗41.018.837.7NEUSUM∗41.5919.0137.98Transformer40.9018.0237.17BERTSUM+Classiﬁer43.2320.2239.60BERTSUM+Transformer43.2520.2439.63BERTSUM+LSTM43.2220.1739.59Table1:TestsetresultsontheCNN/DailyMaildatasetusingROUGEF1.Resultswith∗markaretakenfromthecorrespondingpapers.eachdocument.Thealgorithmgreedilyselectsen-tenceswhichcanmaximizetheROUGEscoresastheoraclesentences.Weassignedlabel1tosen-tencesselectedintheoraclesummaryand0other-wise.4ExperimentalResultsTheexperimentalresultsonCNN/DailymaildatasetsareshowninTable1.Forcomparison,weimplementanon-pretrainedTransformerbase-linewhichusesthesamearchitectureasBERT,butwithsmallerparameters.Itisrandomlyinitial-izedandonlytrainedonthesummarizationtask.TheTransformerbaselinehas6layers,thehiddensizeis512andthefeed-forwardﬁltersizeis2048.Themodelistrainedwithsamesettingsfollow-ingVaswanietal.(2017).Wealsocompareourmodelwithseveralpreviouslyproposedsystems.•LEADisanextractivebaselinewhichusestheﬁrst-3sentencesofthedocumentasasum-mary.•REFRESH(Narayanetal.,2018)isanextrac-tivesummarizationsystemtrainedbyglob-allyoptimizingtheROUGEmetricwithrein-forcementlearning.•NEUSUM(Zhouetal.,2018)isthestate-of-the-artextractivesystemthatjontlyscoreandselectsentences.•PGN(Seeetal.,2017),isthePointerGener-atorNetwork,anabstractivesummarizationsystembasedonanencoder-decoderarchi-tecture.•DCA(Celikyilmazetal.,2018)istheDeepCommunicatingAgents,astate-of-the-artab-stractivesummarizationsystemwithmulti-pleagentstorepresentthedocumentaswellashierarchicalattentionmechanismovertheagentsfordecoding.Asillustratedinthetable,allBERT-basedmod-elsoutperformedpreviousstate-of-the-artmodelsbyalargemargin.BERTSUMwithTransformerachievedthebestperformanceonallthreemet-rics.TheBERTSUMwithLSTMmodeldoesnothaveanobviousinﬂuenceonthesummarizationperformancecomparedtotheClassiﬁermodel.Ablationstudiesareconductedtoshowthecon-tributionofdifferentcomponentsofBERTSUM.TheresultsareshownininTable2.Intervalseg-mentsincreasetheperformanceofbasemodel.Trigramblockingisabletogreatlyimprovethesummarizationresults.Thisisconsistenttopre-viousconclusionsthatasequentialextractivede-coderishelpfultogeneratemoreinformativesum-maries.However,hereweusethetrigramblock-ingasasimplebutrobustalternative.ModelR-1R-2R-LBERTSUM+Classiﬁer43.2320.2239.60-intervalsegments43.2120.1739.57-trigramblocking42.5719.9639.04Table2:ResultsofablationstudiesofBERTSUMonCNN/DailymailtestsetusingROUGEF1(R-1andR-2areshorthandsforunigramandbigramoverlap,R-Listhelongestcommonsubsequence).TheexperimentalresultsonNYTdatasetsareshowninTable3.DifferentfromCNN/Dailymail,weusethelimited-lengthrecallevaluation,fol-lowingDurrettetal.(2016).Wetruncatethepre-dictedsummariestothelengthsofthegoldsum-mariesandevaluatesummarizationqualitywithROUGERecall.Comparedbaselinesare(1)First-kwords,whichisasimplebaselinebyextract-ingﬁrstkwordsoftheinputarticle;(2)Fullisthebest-performedextractivemodelinDur-rettetal.(2016);(3)DeepReinforced(Paulusetal.,2018)isanabstractivemodel,usingrein-forcelearningandencoder-decoderstructure.TheBERTSUM+Classiﬁercanachievethestate-of-the-artresultsonthisdataset.ModelR-1R-2R-LFirst-kwords39.5820.1135.78Full∗42.224.9-DeepReinforced∗42.9426.02-BERTSUM+Classiﬁer46.6626.3542.62Table3:TestsetresultsontheNYT50datasetusingROUGERecall.Thepredictedsummaryaretruncatedtothelengthofthegold-standardsummary.Resultswith∗markaretakenfromthecorrespondingpapers.5ConclusionInthispaper,weexploredhowtouseBERTforextractivesummarization.WeproposedtheBERT-SUMmodelandtriedseveralsummarizationlayerscanbeappliedwithBERT.Wedidexperimentsontwolarge-scaledatasetsandfoundtheBERT-SUMwithinter-sentenceTransformerlayerscanachievethebestperformance.ReferencesJimmyLeiBa,JamieRyanKiros,andGeoffreyEHin-ton.2016.Layernormalization.arXivpreprintarXiv:1607.06450.JaimeGCarbonellandJadeGoldstein.1998.Theuseofmmranddiversity-basedrerankingforreoderingdocumentsandproducingsummaries.AsliCelikyilmaz,AntoineBosselut,XiaodongHe,andYejinChoi.2018.Deepcommunicatingagentsforabstractivesummarization.InProceedingsoftheNAACLConference.MiaXuChen,OrhanFirat,AnkurBapna,MelvinJohnson,WolfgangMacherey,GeorgeFoster,LlionJones,NikiParmar,MikeSchuster,ZhifengChen,etal.2018.Thebestofbothworlds:Combiningrecentadvancesinneuralmachinetranslation.InProceedingsoftheACLConference.JianpengChengandMirellaLapata.2016.Neuralsummarizationbyextractingsentencesandwords.InProceedingsoftheACLConference.JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.2018.Bert:Pre-trainingofdeepbidirectionaltransformersforlanguageunderstand-ing.YueDong,YikangShen,EricCrawford,HerkevanHoof,andJackieChiKitCheung.2018.Banditsum:Extractivesummarizationasacontextualbandit.InProceedingsoftheEMNLPConference.GregDurrett,TaylorBerg-Kirkpatrick,andDanKlein.2016.Learning-basedsingle-documentsummariza-tionwithcompressionandanaphoricityconstraints.InProceedingsoftheACLConference.KarlMoritzHermann,TomasKocisky,EdwardGrefenstette,LasseEspeholt,WillKay,MustafaSu-leyman,andPhilBlunsom.2015.Teachingma-chinestoreadandcomprehend.InAdvancesinNeu-ralInformationProcessingSystems,pages1693–1701.GuillaumeKlein,YoonKim,YuntianDeng,JeanSenellart,andAlexanderMRush.2017.Opennmt:Open-sourcetoolkitforneuralmachinetranslation.InarXivpreprintarXiv:1701.02810.RameshNallapati,FeifeiZhai,andBowenZhou.2017.Summarunner:Arecurrentneuralnetworkbasedse-quencemodelforextractivesummarizationofdocu-ments.InProceedingsoftheAAAIConference.ShashiNarayan,ShayBCohen,andMirellaLapata.2018.Rankingsentencesforextractivesummariza-tionwithreinforcementlearning.InProceedingsoftheNAACLConference.RomainPaulus,CaimingXiong,andRichardSocher.2018.Adeepreinforcedmodelforabstractivesum-marization.InProceedingsoftheICLRConference.EvanSandhaus.2008.TheNewYorkTimesAnnotatedCorpus.LinguisticDataConsortium,Philadelphia,6(12).AbigailSee,PeterJ.Liu,andChristopherD.Manning.2017.Gettothepoint:Summarizationwithpointer-generatornetworks.InProceedingsoftheACLCon-ference.AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,ŁukaszKaiser,andIlliaPolosukhin.2017.Attentionisallyouneed.InAdvancesinNeuralInformationPro-cessingSystems,pages5998–6008.XingxingZhang,MirellaLapata,FuruWei,andMingZhou.2018.Neurallatentextractivedocumentsum-marization.InProceedingsoftheEMNLPConfer-ence.QingyuZhou,NanYang,FuruWei,ShaohanHuang,MingZhou,andTiejunZhao.2018.Neuraldocu-mentsummarizationbyjointlylearningtoscoreandselectsentences.InProceedingsoftheACLConfer-ence.